!
! auto-generated by ops_fortran.py
!
MODULE ADAPTT_KERNEL_ERR_EVAL_MODULE
USE OPS_FORTRAN_DECLARATIONS
USE OPS_FORTRAN_RT_SUPPORT

USE OPS_CONSTANTS
USE ISO_C_BINDING
USE CUDAFOR

real(8), DIMENSION(:), DEVICE, ALLOCATABLE :: reductionArrayDevice4_adaptt_kernel_err_eval
INTEGER(KIND=4), constant :: xdim1_adaptt_kernel_err_eval
INTEGER(KIND=4):: xdim1_adaptt_kernel_err_eval_h  = -1
INTEGER(KIND=4), constant :: ydim1_adaptt_kernel_err_eval
INTEGER(KIND=4):: ydim1_adaptt_kernel_err_eval_h  = -1
INTEGER(KIND=4), constant :: zdim1_adaptt_kernel_err_eval
INTEGER(KIND=4):: zdim1_adaptt_kernel_err_eval_h  = -1
#define OPS_ACC1(x,y,z) (x+xdim1_adaptt_kernel_err_eval*(y)+xdim1_adaptt_kernel_err_eval*ydim1_adaptt_kernel_err_eval*(z)+1)
INTEGER(KIND=4), constant :: xdim2_adaptt_kernel_err_eval
INTEGER(KIND=4):: xdim2_adaptt_kernel_err_eval_h  = -1
INTEGER(KIND=4), constant :: ydim2_adaptt_kernel_err_eval
INTEGER(KIND=4):: ydim2_adaptt_kernel_err_eval_h  = -1
INTEGER(KIND=4), constant :: zdim2_adaptt_kernel_err_eval
INTEGER(KIND=4):: zdim2_adaptt_kernel_err_eval_h  = -1
#define OPS_ACC2(x,y,z) (x+xdim2_adaptt_kernel_err_eval*(y)+xdim2_adaptt_kernel_err_eval*ydim2_adaptt_kernel_err_eval*(z)+1)


contains

!Reduction cuda kernel
attributes (device) SUBROUTINE ReductionFloat8(sharedDouble8, reductionResult,inputValue,reductionOperation)
  REAL(kind=8), DIMENSION(:), DEVICE :: reductionResult
  REAL(kind=8) :: inputValue
  INTEGER(kind=4), VALUE :: reductionOperation
  REAL(kind=8), DIMENSION(0:*) :: sharedDouble8
  INTEGER(kind=4) :: i1
  INTEGER(kind=4) :: threadID
  threadID = (threadIdx%y-1)*blockDim%x + (threadIdx%x - 1)
  i1 = ishft(blockDim%x*blockDim%y,-1)
  CALL syncthreads()
  sharedDouble8(threadID) = inputValue
  DO WHILE (i1 > 0 )
    CALL syncthreads()
    IF (threadID < i1) THEN
      SELECT CASE(reductionOperation)
      CASE (0)
      sharedDouble8(threadID) = sharedDouble8(threadID) + sharedDouble8(threadID + i1)
      CASE (1)
      IF (sharedDouble8(threadID + i1) < sharedDouble8(threadID)) THEN
        sharedDouble8(threadID) = sharedDouble8(threadID + i1)
      ENDIF
      CASE (2)
      IF (sharedDouble8(threadID + i1) > sharedDouble8(threadID)) THEN
        sharedDouble8(threadID) = sharedDouble8(threadID + i1)
      ENDIF
      END SELECT
    ENDIF
    i1 = ishft(i1,-1)
  END DO
  CALL syncthreads()
  IF (threadID .EQ. 0) THEN
    SELECT CASE(reductionOperation)
    CASE (0)
    reductionResult(1) = reductionResult(1) + sharedDouble8(0)
    CASE (1)
    IF (sharedDouble8(0) < reductionResult(1)) THEN
      reductionResult(1) = sharedDouble8(0)
    ENDIF
    CASE (2)
    IF (sharedDouble8(0) > reductionResult(1)) THEN
      reductionResult(1) = sharedDouble8(0)
    ENDIF
    END SELECT
  ENDIF
  CALL syncthreads()
  END SUBROUTINE

  attributes (device) SUBROUTINE ReductionInt4(sharedInt4, reductionResult,inputValue,reductionOperation)
  INTEGER(kind=4), DIMENSION(:), DEVICE :: reductionResult
  INTEGER(kind=4) :: inputValue
  INTEGER(kind=4), VALUE :: reductionOperation
  INTEGER(kind=4), DIMENSION(0:*) :: sharedInt4
  INTEGER(kind=4) :: i1
  INTEGER(kind=4) :: threadID
  threadID = (threadIdx%y-1)*blockDim%x + (threadIdx%x - 1)
  i1 = ishft(blockDim%x*blockDim%y,-1)
  CALL syncthreads()
  sharedInt4(threadID) = inputValue
  DO WHILE (i1 > 0 )
    CALL syncthreads()
    IF (threadID < i1) THEN
      SELECT CASE(reductionOperation)
      CASE (0)
      sharedInt4(threadID) = sharedInt4(threadID) + sharedInt4(threadID + i1)
      CASE (1)
      IF (sharedInt4(threadID + i1) < sharedInt4(threadID)) THEN
        sharedInt4(threadID) = sharedInt4(threadID + i1)
      ENDIF
      CASE (2)
      IF (sharedInt4(threadID + i1) > sharedInt4(threadID)) THEN
        sharedInt4(threadID) = sharedInt4(threadID + i1)
      ENDIF
      END SELECT
    ENDIF
    i1 = ishft(i1,-1)
  END DO
  CALL syncthreads()
  IF (threadID .EQ. 0) THEN
    SELECT CASE(reductionOperation)
    CASE (0)
    reductionResult(1) = reductionResult(1) + sharedInt4(0)
    CASE (1)
    IF (sharedInt4(0) < reductionResult(1)) THEN
      reductionResult(1) = sharedInt4(0)
    ENDIF
    CASE (2)
    IF (sharedInt4(0) > reductionResult(1)) THEN
      reductionResult(1) = sharedInt4(0)
    ENDIF
    END SELECT
  ENDIF
  CALL syncthreads()
END SUBROUTINE

!user function
attributes (device) SUBROUTINE adaptt_kernel_err_eval_gpu(err_arr, run_arr, ernrm, ertot)
    use data_types
    implicit none

    real(kind=8), dimension(1), intent(in) :: err_arr, run_arr
    real(kind=8), intent(in) :: ernrm
    real(kind=8) :: ertot
    real(kind=8) :: fornow

    fornow = abs(err_arr(OPS_ACC1(0,0,0)))/(abs(run_arr(OPS_ACC2(0,0,0)))+ernrm)
    ertot = max( ertot, fornow )

END SUBROUTINE

#undef OPS_ACC1
#undef OPS_ACC2



!CUDA kernel function -- wrapper calling user kernel
attributes (global) subroutine adaptt_kernel_err_eval_wrap( &
& opsDat1Local, &
& opsDat2Local, &
& opsGblDat3Device,   &
& reductionArrayDevice4,   &
& dat1_base, &
& dat2_base, &
& size1, size2, size3 )
  IMPLICIT NONE
  real(8), DEVICE, INTENT(IN) :: opsDat1Local(*)
  integer(4) arg1
  real(8), DEVICE, INTENT(IN) :: opsDat2Local(*)
  integer(4) arg2
  real(8), VALUE :: opsGblDat3Device
  real(8), DIMENSION(:), DEVICE :: reductionArrayDevice4
  real(8) :: opsGblDat4Device
  real(8), DIMENSION(0:*), SHARED :: sharedMem
  integer(4), value :: dat1_base
  integer(4), value :: dat2_base
  integer(4) start(3)
  integer(4) end(3)
  integer, value :: size1,size2,size3
  integer n_x, n_y, n_z


  n_z = blockDim%z * (blockIdx%z-1) + threadIdx%z
  n_y = blockDim%y * (blockIdx%y-1) + threadIdx%y
  n_x = blockDim%x * (blockIdx%x-1) + threadIdx%x

  arg1 = (n_x-1) * 1*1 + (n_y-1) * 1*1 * xdim1_adaptt_kernel_err_eval + (n_z-1) * 1*1 * xdim1_adaptt_kernel_err_eval * ydim1_adaptt_kernel_err_eval
  arg2 = (n_x-1) * 1*1 + (n_y-1) * 1*1 * xdim2_adaptt_kernel_err_eval + (n_z-1) * 1*1 * xdim2_adaptt_kernel_err_eval * ydim2_adaptt_kernel_err_eval
  opsGblDat4Device = -1.0_8*HUGE(opsGblDat4Device)
  IF ((n_x-1) < size1 .AND. (n_y-1) < size2 .AND. (n_z-1) < size3) THEN
    call adaptt_kernel_err_eval_gpu( &
    & opsDat1Local(dat1_base+arg1), &
    & opsDat2Local(dat2_base+arg2), &
    & opsGblDat3Device, &
    & opsGblDat4Device )

  ENDIF

  call ReductionFloat8(sharedMem, reductionArrayDevice4((blockIdx%z - 1)*gridDim%y*gridDim%x + (blockIdx%y - 1)*gridDim%x + (blockIdx%x-1) + 1:),opsGblDat4Device,2)

end subroutine

!host subroutine
attributes (host) subroutine adaptt_kernel_err_eval_host( userSubroutine, block, dim, range, &
& opsArg1, &
& opsArg2, &
& opsArg3, &
& opsArg4)
  use cudafor
  IMPLICIT NONE
  character(kind=c_char,len=*), INTENT(IN) :: userSubroutine
  type ( ops_block ), INTENT(IN) :: block
  integer(kind=4), INTENT(IN):: dim
  integer(kind=4)   , DIMENSION(2*dim), INTENT(IN) :: range
  real(kind=8) t1,t2,t3
  real(kind=4) transfer_total, transfer
  integer(kind=4) :: istat

  type ( ops_arg )  , INTENT(IN) :: opsArg1
  real(8), DIMENSION(:), DEVICE, POINTER  :: opsDat1Local
  integer(kind=4) :: opsDat1Cardinality
  integer(kind=4), POINTER, DIMENSION(:)  :: dat1_size
  integer(kind=4) :: dat1_base
  INTEGER(KIND=4) :: xdim1
  INTEGER(KIND=4) :: ydim1, zdim1

  type ( ops_arg )  , INTENT(IN) :: opsArg2
  real(8), DIMENSION(:), DEVICE, POINTER  :: opsDat2Local
  integer(kind=4) :: opsDat2Cardinality
  integer(kind=4), POINTER, DIMENSION(:)  :: dat2_size
  integer(kind=4) :: dat2_base
  INTEGER(KIND=4) :: xdim2
  INTEGER(KIND=4) :: ydim2, zdim2

  type ( ops_arg )  , INTENT(IN) :: opsArg3
  integer(kind=4) :: opsDat3Cardinality
  real(8), DIMENSION(:), POINTER :: opsDat3Host
  type ( ops_arg )  , INTENT(IN) :: opsArg4
  integer(kind=4) :: opsDat4Cardinality
  real(8), DIMENSION(:), POINTER :: opsDat4Host
  real(8), DIMENSION(:), ALLOCATABLE :: reductionArrayHost4
  INTEGER(kind=4) :: reductionCardinality4

  integer x_size, y_size, z_size
  integer start(3)
  integer end(3)
  integer(kind=4) :: n
  integer(kind=4) :: i10
  integer(kind=4) :: i20
  integer(kind=4) :: blocksPerGrid
  integer(kind=4) :: nshared
  integer(kind=4) :: nthread

  !cuda grid and thread block sizes
  type(dim3) :: grid, tblock

  type ( ops_arg ) , DIMENSION(4) :: opsArgArray

  opsArgArray(1) = opsArg1
  opsArgArray(2) = opsArg2
  opsArgArray(3) = opsArg3
  opsArgArray(4) = opsArg4

  call setKernelTime(316,userSubroutine//char(0),0.0_8,0.0_8,0.0_4,1)
  call ops_timers_core(t1)

#ifdef OPS_MPI
  IF (getRange(block, start, end, range) < 0) THEN
    return
  ENDIF
#else
  DO n = 1, 3
    start(n) = range(2*n-1)
    end(n) = range(2*n)
  END DO
#endif


  x_size = MAX(0,end(1)-start(1)+1)
  y_size = MAX(0,end(2)-start(2)+1)
  z_size = MAX(0,end(3)-start(3)+1)

  call c_f_pointer(getDatSizeFromOpsArg(opsArg1),dat1_size,(/dim/))
  xdim1 = dat1_size(1)
  ydim1 = dat1_size(2)
  zdim1 = dat1_size(3)
  opsDat1Cardinality = opsArg1%dim * xdim1 * ydim1 * zdim1
  dat1_base = getDatBaseFromOpsArg3D(opsArg1,start,1)
  call c_f_pointer(opsArg1%data_d,opsDat1Local,(/opsDat1Cardinality/))

  call c_f_pointer(getDatSizeFromOpsArg(opsArg2),dat2_size,(/dim/))
  xdim2 = dat2_size(1)
  ydim2 = dat2_size(2)
  zdim2 = dat2_size(3)
  opsDat2Cardinality = opsArg2%dim * xdim2 * ydim2 * zdim2
  dat2_base = getDatBaseFromOpsArg3D(opsArg2,start,1)
  call c_f_pointer(opsArg2%data_d,opsDat2Local,(/opsDat2Cardinality/))

  opsDat3Cardinality = opsArg3%dim
  call c_f_pointer(opsArg3%data,opsDat3Host,(/opsDat3Cardinality/))

  opsDat4Cardinality = opsArg4%dim
  call c_f_pointer(getReductionPtrFromOpsArg(opsArg4,block),opsDat4Host,(/opsDat4Cardinality/))

  IF ((xdim1 .NE. xdim1_adaptt_kernel_err_eval_h) .OR. &
  (ydim1 .NE. ydim1_adaptt_kernel_err_eval_h) .OR. &
  (xdim2 .NE. xdim2_adaptt_kernel_err_eval_h) .OR. &
  (ydim2 .NE. ydim2_adaptt_kernel_err_eval_h) ) THEN
    xdim1_adaptt_kernel_err_eval = xdim1
    xdim1_adaptt_kernel_err_eval_h = xdim1
    ydim1_adaptt_kernel_err_eval = ydim1
    ydim1_adaptt_kernel_err_eval_h = ydim1
    xdim2_adaptt_kernel_err_eval = xdim2
    xdim2_adaptt_kernel_err_eval_h = xdim2
    ydim2_adaptt_kernel_err_eval = ydim2
    ydim2_adaptt_kernel_err_eval_h = ydim2
  ENDIF

  grid = dim3( (x_size-1)/getOPS_block_size_x()+ 1, (y_size-1)/getOPS_block_size_y() + 1, z_size)
  tblock = dim3(getOPS_block_size_x(),getOPS_block_size_y(),1)

  !Reduction vars and shared memory for reductions
  nshared = 0
  nthread = getOPS_block_size_x()*getOPS_block_size_y()
  blocksPerGrid = ((x_size-1)/getOPS_block_size_x()+ 1)*((y_size-1)/getOPS_block_size_y() + 1)* z_size

  nshared = MAX(nshared,8*1*nthread)

  reductionCardinality4 = blocksPerGrid * 1
  allocate( reductionArrayHost4(reductionCardinality4* (1)) )
  IF (.not. allocated(reductionArrayDevice4_adaptt_kernel_err_eval)) THEN
    allocate( reductionArrayDevice4_adaptt_kernel_err_eval(reductionCardinality4* (1)) )
  ENDIF

  DO i10 = 0, reductionCardinality4-1
    reductionArrayHost4(i10+1) = -1.0*HUGE(reductionArrayHost4(1))
  END DO

  reductionArrayDevice4_adaptt_kernel_err_eval = reductionArrayHost4

  !halo exchanges
  call ops_H_D_exchanges_device(opsArgArray,4)
  call ops_halo_exchanges(opsArgArray,4,range)
  call ops_H_D_exchanges_device(opsArgArray,4)

  call ops_timers_core(t2)
  call adaptt_kernel_err_eval_wrap <<<grid,tblock,nshared>>> (&
  & opsDat1Local, &
  & opsDat2Local, &
  & opsDat3Host(1), &
  & reductionArrayDevice4_adaptt_kernel_err_eval, &
  & dat1_base, &
  & dat2_base, &
  & x_size, y_size, z_size )

  reductionArrayHost4 = reductionArrayDevice4_adaptt_kernel_err_eval

  DO i10 = 0, reductionCardinality4-1
    opsDat4Host = max(opsDat4Host, reductionArrayHost4(i10+1))
  END DO

  deallocate( reductionArrayHost4 )
  istat = cudaDeviceSynchronize()
  call ops_timers_core(t3)
  call ops_set_dirtybit_device(opsArgArray, 4)

  !Timing and data movement
  transfer_total = 0.0_4
  call ops_compute_transfer(3, start, end, opsArg1,transfer)
  transfer_total = transfer_total + transfer
  call ops_compute_transfer(3, start, end, opsArg2,transfer)
  transfer_total = transfer_total + transfer
  call setKernelTime(316,userSubroutine,t3-t2,t2-t1,transfer_total,0)
end subroutine
END MODULE
