!
! auto-generated by ops_fortran.py
!

MODULE ADAPTT_KERNEL_ERR_EVAL_MODULE
USE OPS_FORTRAN_DECLARATIONS
USE OPS_FORTRAN_RT_SUPPORT

USE OPS_CONSTANTS
USE ISO_C_BINDING
USE CUDAFOR

real(kind=8), dimension(:), device, allocatable :: reductionArrayDevice4_adaptt_kernel_err_eval
integer(4), constant :: xdim1_adaptt_kernel_err_eval
integer(4):: xdim1_adaptt_kernel_err_eval_h  = -1
integer(4), constant :: ydim1_adaptt_kernel_err_eval
integer(4):: ydim1_adaptt_kernel_err_eval_h  = -1
#define OPS_ACC1(x,y,z) (x+xdim1_adaptt_kernel_err_eval*(y)+xdim1_adaptt_kernel_err_eval*ydim1_adaptt_kernel_err_eval*(z)+1)

integer(4), constant :: xdim2_adaptt_kernel_err_eval
integer(4):: xdim2_adaptt_kernel_err_eval_h  = -1
integer(4), constant :: ydim2_adaptt_kernel_err_eval
integer(4):: ydim2_adaptt_kernel_err_eval_h  = -1
#define OPS_ACC2(x,y,z) (x+xdim2_adaptt_kernel_err_eval*(y)+xdim2_adaptt_kernel_err_eval*ydim2_adaptt_kernel_err_eval*(z)+1)



contains

!Reduction cuda kernel
attributes (device) SUBROUTINE ReductionFloat8(sharedDouble8, reductionResult,inputValue,reductionOperation)
  REAL(8), dimension(:), DEVICE :: reductionResult
  REAL(8) :: inputValue
  integer(4), VALUE :: reductionOperation
  REAL(8), dimension(0:*) :: sharedDouble8
  integer(4) :: i1
  integer(4) :: threadID
  threadID = (threadIdx%y-1)*blockDim%x + (threadIdx%x - 1)
  i1 = ishft(blockDim%x*blockDim%y,-1)
  CALL syncthreads()
  sharedDouble8(threadID) = inputValue
  DO WHILE (i1 > 0 )
    CALL syncthreads()
    IF (threadID < i1) THEN
      SELECT CASE(reductionOperation)
      CASE (0)
      sharedDouble8(threadID) = sharedDouble8(threadID) + sharedDouble8(threadID + i1)
      CASE (1)
      IF (sharedDouble8(threadID + i1) < sharedDouble8(threadID)) THEN
        sharedDouble8(threadID) = sharedDouble8(threadID + i1)
      ENDIF
      CASE (2)
      IF (sharedDouble8(threadID + i1) > sharedDouble8(threadID)) THEN
        sharedDouble8(threadID) = sharedDouble8(threadID + i1)
      ENDIF
      END SELECT
    ENDIF
    i1 = ishft(i1,-1)
  END DO
  CALL syncthreads()
  IF (threadID .EQ. 0) THEN
    SELECT CASE(reductionOperation)
    CASE (0)
    reductionResult(1) = reductionResult(1) + sharedDouble8(0)
    CASE (1)
    IF (sharedDouble8(0) < reductionResult(1)) THEN
      reductionResult(1) = sharedDouble8(0)
    ENDIF
    CASE (2)
    IF (sharedDouble8(0) > reductionResult(1)) THEN
      reductionResult(1) = sharedDouble8(0)
    ENDIF
    END SELECT
  ENDIF
  CALL syncthreads()
  END SUBROUTINE

  attributes (device) SUBROUTINE ReductionInt4(sharedInt4, reductionResult,inputValue,reductionOperation)
  integer(4), dimension(:), DEVICE :: reductionResult
  integer(4) :: inputValue
  integer(4), VALUE :: reductionOperation
  integer(4), dimension(0:*) :: sharedInt4
  integer(4) :: i1
  integer(4) :: threadID
  threadID = (threadIdx%y-1)*blockDim%x + (threadIdx%x - 1)
  i1 = ishft(blockDim%x*blockDim%y,-1)
  CALL syncthreads()
  sharedInt4(threadID) = inputValue
  DO WHILE (i1 > 0 )
    CALL syncthreads()
    IF (threadID < i1) THEN
      SELECT CASE(reductionOperation)
      CASE (0)
      sharedInt4(threadID) = sharedInt4(threadID) + sharedInt4(threadID + i1)
      CASE (1)
      IF (sharedInt4(threadID + i1) < sharedInt4(threadID)) THEN
        sharedInt4(threadID) = sharedInt4(threadID + i1)
      ENDIF
      CASE (2)
      IF (sharedInt4(threadID + i1) > sharedInt4(threadID)) THEN
        sharedInt4(threadID) = sharedInt4(threadID + i1)
      ENDIF
      END SELECT
    ENDIF
    i1 = ishft(i1,-1)
  END DO
  CALL syncthreads()
  IF (threadID .EQ. 0) THEN
    SELECT CASE(reductionOperation)
    CASE (0)
    reductionResult(1) = reductionResult(1) + sharedInt4(0)
    CASE (1)
    IF (sharedInt4(0) < reductionResult(1)) THEN
      reductionResult(1) = sharedInt4(0)
    ENDIF
    CASE (2)
    IF (sharedInt4(0) > reductionResult(1)) THEN
      reductionResult(1) = sharedInt4(0)
    ENDIF
    END SELECT
  ENDIF
  CALL syncthreads()
END SUBROUTINE

!user function
attributes (device) SUBROUTINE adaptt_kernel_err_eval_gpu(err_arr, run_arr, ernrm, ertot)
    use data_types
    implicit none

    real(kind=8), dimension(1), intent(in) :: err_arr, run_arr
    real(kind=8), intent(in) :: ernrm
    real(kind=8) :: ertot
    real(kind=8) :: fornow

    fornow = abs(err_arr(OPS_ACC1(0,0,0)))/(abs(run_arr(OPS_ACC2(0,0,0)))+ernrm)
    ertot = max( ertot, fornow )

END SUBROUTINE

#undef OPS_ACC1
#undef OPS_ACC2


!CUDA kernel function -- wrapper calling user kernel
attributes (global) subroutine adaptt_kernel_err_eval_wrap( &
& opsDat1Local, &
& opsDat2Local, &
& opsGblDat3Device,   &
& reductionArrayDevice4,   &
& dat1_base, &
& dat2_base, &
& size1, size2, size3 )

  IMPLICIT NONE

  real(kind=8), device, dimension(*), intent(in)    :: opsDat1Local
  integer(4) :: arg1
  real(kind=8), device, dimension(*), intent(in)    :: opsDat2Local
  integer(4) :: arg2
  real(kind=8), value :: opsGblDat3Device
  real(kind=8), dimension(:), device :: reductionArrayDevice4
  real(kind=8) :: opsGblDat4Device
  real(kind=8), dimension(0:*), shared :: sharedMem
  integer(4), value :: dat1_base
  integer(4), value :: dat2_base
  integer(4), value :: size1, size2, size3
  integer(4)        :: n_x, n_y, n_z


  n_z = blockDim%z * (blockIdx%z-1) + threadIdx%z
  n_y = blockDim%y * (blockIdx%y-1) + threadIdx%y
  n_x = blockDim%x * (blockIdx%x-1) + threadIdx%x

  arg1 = (n_x-1) * 1*1 + (n_y-1) * 1*1 * xdim1_adaptt_kernel_err_eval + (n_z-1) * 1*1 * xdim1_adaptt_kernel_err_eval * ydim1_adaptt_kernel_err_eval
  arg2 = (n_x-1) * 1*1 + (n_y-1) * 1*1 * xdim2_adaptt_kernel_err_eval + (n_z-1) * 1*1 * xdim2_adaptt_kernel_err_eval * ydim2_adaptt_kernel_err_eval
  opsGblDat4Device = -1.0_8*HUGE(opsGblDat4Device)
  IF ((n_x-1) < size1 .AND. (n_y-1) < size2 .AND. (n_z-1) < size3) THEN

    call adaptt_kernel_err_eval_gpu( &
    & opsDat1Local(dat1_base+arg1), &
    & opsDat2Local(dat2_base+arg2), &
    & opsGblDat3Device, &
    & opsGblDat4Device )

  ENDIF

  call ReductionFloat8(sharedMem, reductionArrayDevice4((blockIdx%z - 1)*gridDim%y*gridDim%x + (blockIdx%y - 1)*gridDim%x + (blockIdx%x-1) + 1:),opsGblDat4Device,2)

end subroutine

!host subroutine
attributes (host) subroutine adaptt_kernel_err_eval_host( userSubroutine, block, dim, range, &
& opsArg1, &
& opsArg2, &
& opsArg3, &
& opsArg4)

  USE CUDAFOR
  IMPLICIT NONE

  character(kind=c_char,len=*), intent(in) :: userSubroutine
  type(ops_block), intent(in) :: block
  integer(4), intent(in):: dim
  integer(4), dimension(2*dim), intent(in) :: range
  real(8) :: t1,t2,t3
  real(4) :: transfer_total, transfer
  integer(4) :: istat

  type(ops_arg), intent(in) :: opsArg1
  real(kind=8), dimension(:), device, pointer  :: opsDat1Local
  integer(4) :: opsDat1Cardinality
  integer(4), pointer, dimension(:) :: dat1_size
  integer(4) :: dat1_base
  integer(4) :: xdim1
  integer(4) :: ydim1, zdim1

  type(ops_arg), intent(in) :: opsArg2
  real(kind=8), dimension(:), device, pointer  :: opsDat2Local
  integer(4) :: opsDat2Cardinality
  integer(4), pointer, dimension(:) :: dat2_size
  integer(4) :: dat2_base
  integer(4) :: xdim2
  integer(4) :: ydim2, zdim2

  type(ops_arg), intent(in) :: opsArg3
  integer(4) :: opsDat3Cardinality
  real(kind=8), dimension(:), pointer :: opsDat3Host
  type(ops_arg), intent(in) :: opsArg4
  integer(4) :: opsDat4Cardinality
  real(kind=8), dimension(:), pointer :: opsDat4Host
  real(kind=8), dimension(:), allocatable :: reductionArrayHost4
  integer(4) :: reductionCardinality4

  integer(4) :: x_size, y_size, z_size
  integer(4), dimension(3) :: start_indx, end_indx
  integer(4) :: n
  integer(4) :: i10
  integer(4) :: i20
  integer(4) :: blocksPerGrid
  integer(4) :: nshared
  integer(4) :: nthread

  !cuda grid and thread block sizes
  type(dim3) :: grid, tblock

  type(ops_arg), dimension(4) :: opsArgArray

  opsArgArray(1) = opsArg1
  opsArgArray(2) = opsArg2
  opsArgArray(3) = opsArg3
  opsArgArray(4) = opsArg4

  call setKernelTime(308,userSubroutine//char(0),0.0_8,0.0_8,0.0_4,1)
  call ops_timers_core(t1)

#ifdef OPS_MPI
  IF (getRange(block, start_indx, end_indx, range) < 0) THEN
    return
  ENDIF
#else
  DO n = 1, 3
    start_indx(n) = range(2*n-1)
    end_indx(n)   = range(2*n)
  END DO
#endif


  x_size = MAX(0,end_indx(1)-start_indx(1)+1)
  y_size = MAX(0,end_indx(2)-start_indx(2)+1)
  z_size = MAX(0,end_indx(3)-start_indx(3)+1)

  call c_f_pointer(getDatSizeFromOpsArg(opsArg1),dat1_size,(/dim/))
  xdim1 = dat1_size(1)
  ydim1 = dat1_size(2)
  zdim1 = dat1_size(3)
  opsDat1Cardinality = opsArg1%dim * xdim1 * ydim1 * zdim1
  dat1_base = getDatBaseFromOpsArg3D(opsArg1,start_indx,1)
  call c_f_pointer(opsArg1%data_d,opsDat1Local,(/opsDat1Cardinality/))

  call c_f_pointer(getDatSizeFromOpsArg(opsArg2),dat2_size,(/dim/))
  xdim2 = dat2_size(1)
  ydim2 = dat2_size(2)
  zdim2 = dat2_size(3)
  opsDat2Cardinality = opsArg2%dim * xdim2 * ydim2 * zdim2
  dat2_base = getDatBaseFromOpsArg3D(opsArg2,start_indx,1)
  call c_f_pointer(opsArg2%data_d,opsDat2Local,(/opsDat2Cardinality/))

  opsDat3Cardinality = opsArg3%dim
  call c_f_pointer(opsArg3%data,opsDat3Host,(/opsDat3Cardinality/))

  opsDat4Cardinality = opsArg4%dim
  call c_f_pointer(getReductionPtrFromOpsArg(opsArg4,block),opsDat4Host,(/opsDat4Cardinality/))

  IF ((xdim1 .NE. xdim1_adaptt_kernel_err_eval_h) .OR. &
  (ydim1 .NE. ydim1_adaptt_kernel_err_eval_h) .OR. &
  (xdim2 .NE. xdim2_adaptt_kernel_err_eval_h) .OR. &
  (ydim2 .NE. ydim2_adaptt_kernel_err_eval_h) ) THEN
    xdim1_adaptt_kernel_err_eval = xdim1
    xdim1_adaptt_kernel_err_eval_h = xdim1
    ydim1_adaptt_kernel_err_eval = ydim1
    ydim1_adaptt_kernel_err_eval_h = ydim1
    xdim2_adaptt_kernel_err_eval = xdim2
    xdim2_adaptt_kernel_err_eval_h = xdim2
    ydim2_adaptt_kernel_err_eval = ydim2
    ydim2_adaptt_kernel_err_eval_h = ydim2
  ENDIF

  grid = dim3( (x_size-1)/getOPS_block_size_x()+ 1, (y_size-1)/getOPS_block_size_y() + 1, z_size)
  tblock = dim3(getOPS_block_size_x(),getOPS_block_size_y(),1)

  !Reduction vars and shared memory for reductions
  nshared = 0
  nthread = getOPS_block_size_x()*getOPS_block_size_y()
  blocksPerGrid = ((x_size-1)/getOPS_block_size_x()+ 1)*((y_size-1)/getOPS_block_size_y() + 1)* z_size

  nshared = MAX(nshared,8*1*nthread)

  reductionCardinality4 = blocksPerGrid * 1
  allocate( reductionArrayHost4(reductionCardinality4* (1)) )
  IF (.not. allocated(reductionArrayDevice4_adaptt_kernel_err_eval)) THEN
    allocate( reductionArrayDevice4_adaptt_kernel_err_eval(reductionCardinality4* (1)) )
  ENDIF

  DO i10 = 0, reductionCardinality4-1
    reductionArrayHost4(i10+1) = -1.0*HUGE(reductionArrayHost4(1))
  END DO

  reductionArrayDevice4_adaptt_kernel_err_eval = reductionArrayHost4

  !halo exchanges
  call ops_H_D_exchanges_device(opsArgArray,4)
  call ops_halo_exchanges(opsArgArray,4,range)
  call ops_H_D_exchanges_device(opsArgArray,4)

  call ops_timers_core(t2)

  call adaptt_kernel_err_eval_wrap <<<grid,tblock,nshared>>> (&
  & opsDat1Local, &
  & opsDat2Local, &
  & opsDat3Host(1), &
  & reductionArrayDevice4_adaptt_kernel_err_eval, &
  & dat1_base, &
  & dat2_base, &
  & x_size, y_size, z_size )

  reductionArrayHost4 = reductionArrayDevice4_adaptt_kernel_err_eval

  DO i10 = 0, reductionCardinality4-1
    opsDat4Host = max(opsDat4Host, reductionArrayHost4(i10+1))
  END DO

  deallocate( reductionArrayHost4 )
  istat = cudaDeviceSynchronize()
  call ops_timers_core(t3)
  call ops_set_dirtybit_device(opsArgArray, 4)

  !Timing and data movement
  transfer_total = 0.0_4
  call ops_compute_transfer(3, start_indx, end_indx, opsArg1,transfer)
  transfer_total = transfer_total + transfer
  call ops_compute_transfer(3, start_indx, end_indx, opsArg2,transfer)
  transfer_total = transfer_total + transfer
  call setKernelTime(308,userSubroutine,t3-t2,t2-t1,transfer_total,0)

end subroutine

END MODULE
